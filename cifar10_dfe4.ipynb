{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_dfe4.ipynb",
      "provenance": [],
      "mount_file_id": "1VKCdW34aoAskurkaJCWp2k8RZc9FGING",
      "authorship_tag": "ABX9TyM5+9LYJ102THr4lV+9EiaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ba7e28228c4d43b581c26c727ea6d0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8bc6caf45ffc495f979d9850decfa0ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_421b63629ee44dfa80d65831825e91af",
              "IPY_MODEL_798df22772de436380d449512a87fda3"
            ]
          }
        },
        "8bc6caf45ffc495f979d9850decfa0ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "421b63629ee44dfa80d65831825e91af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75fa34cd1090415bbf8061f660bd1450",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b63c936928d347ce8f4126ac27286c1a"
          }
        },
        "798df22772de436380d449512a87fda3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_56a5788d7e01468ebbcfcc39c1e14249",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:07&lt;00:00, 23949412.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd169ad817074a91a3b973484e73227a"
          }
        },
        "75fa34cd1090415bbf8061f660bd1450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b63c936928d347ce8f4126ac27286c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56a5788d7e01468ebbcfcc39c1e14249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd169ad817074a91a3b973484e73227a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongilp/CIFAR10/blob/main/cifar10_dfe4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D0SugdOJd1-"
      },
      "source": [
        "class Config():\n",
        "    def __init__(self):\n",
        "        self.modelname = \"ResNet32\"  # MLP / LeNet5 / ResNet32\n",
        "        self.batch_size = 128\n",
        "        self.lr = 0.1\n",
        "        self.momentum = 0.9\n",
        "        self.weight_decay = 1e-04\n",
        "        self.finish_step = 15000"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2NdLacbKgLr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LeNet5_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        # input channels, output channels, kernel size\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # kernel size, stride, padding = 0 (default)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # input features, output features\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHx6-eWBKhvv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class IdentityPadding(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.pooling = nn.MaxPool2d(kernel_size=1, stride=stride)\n",
        "        self.add_channels = out_channels - in_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 패딩전 x: torch.Size([200, 32, 16, 16])\n",
        "        x = F.pad(x, [0, 0, 0, 0, 0, self.add_channels])\n",
        "        # 패딩후 x: torch.Size([200, 64, 16, 16])\n",
        "        x = self.pooling(x)\n",
        "        # 풀링후 x: torch.Size([200, 64, 8, 8])\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, down_sample=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "        if down_sample:\n",
        "            self.down_sample = IdentityPadding(in_channels, out_channels, stride)\n",
        "        else:\n",
        "            self.down_sample = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.down_sample is not None:\n",
        "            shortcut = self.down_sample(shortcut)\n",
        "\n",
        "        x += shortcut\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, num_classes=10):\n",
        "        super().__init__()\n",
        "        # input img : [3, 32, 32]\n",
        "        self.num_layers = num_layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,\n",
        "                               kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        block = ResidualBlock\n",
        "\n",
        "        # feature map size = [16,32,32]\n",
        "        self.layers_2n = self.get_layers(block, 16, 16, stride=1)\n",
        "        # feature map size = [32,16,16]\n",
        "        self.layers_4n = self.get_layers(block, 16, 32, stride=2)\n",
        "        # feature map size = [64,8,8]\n",
        "        self.layers_6n = self.get_layers(block, 32, 64, stride=2)\n",
        "\n",
        "        # output layers\n",
        "        self.pool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc_out = nn.Linear(64, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def get_layers(self, block, in_channels, out_channels, stride):\n",
        "        if stride == 2:\n",
        "            down_sample = True\n",
        "        else:\n",
        "            down_sample = False\n",
        "\n",
        "        layers_list = nn.ModuleList([block(in_channels, out_channels, stride, down_sample)])\n",
        "\n",
        "        for _ in range(self.num_layers - 1):\n",
        "            layers_list.append(block(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layers_2n(x)\n",
        "        x = self.layers_4n(x)\n",
        "        x = self.layers_6n(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNet32_model():\n",
        "    block = ResidualBlock\n",
        "    model = ResNet(5)\n",
        "    return model\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMDBHbgIKlOP"
      },
      "source": [
        "import torchvision.utils as utils\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def recall_model(cfg, save_path = None):\n",
        "    if cfg.modelname == \"LeNet5\":\n",
        "        model = LeNet5_model()\n",
        "    elif cfg.modelname == \"ResNet32\":\n",
        "        model = ResNet32_model()\n",
        "    else:\n",
        "        print(\"Wrong modelname.\")\n",
        "        quit()\n",
        "\n",
        "    if save_path is not None:\n",
        "        checkpoint = torch.load(save_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        return model, checkpoint\n",
        "\n",
        "    return model\n",
        "\n",
        "def imgshow(image, label, classes):\n",
        "    print('========================================')\n",
        "    print(\"The 1st image:\")\n",
        "    print(image)\n",
        "    print('Shape of this image\\t:', image.shape)\n",
        "    plt.imshow(np.transpose(image, (1, 2, 0)))\n",
        "    plt.title('Label:%s' % classes[label])\n",
        "    plt.show()\n",
        "    print('Label of this image:', label, classes[label])\n",
        "\n",
        "def evaluate(model, test_loader, device, verbose = False):\n",
        "    correct_cnt = 0\n",
        "    model.eval()\n",
        "    for img, label in test_loader:\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model.forward(img)\n",
        "        _, top_pred = torch.topk(pred, k = 1, dim = -1)\n",
        "        top_pred = top_pred.squeeze(dim=1)\n",
        "        if verbose:\n",
        "            print(\"--------------------------------------\")\n",
        "            print(\"truth:\", classes[label])\n",
        "            print(\"model prediction:\", classes[top_pred])\n",
        "\n",
        "        correct_cnt += int(torch.sum(top_pred == label))\n",
        "\n",
        "    return correct_cnt\n",
        "\n",
        "def train_data_load():\n",
        "    # train data augmentation : 1) 데이터 좌우반전(2배). 2) size 4만큼 패딩 후 32의 크기로 random cropping\n",
        "    transforms_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    transforms_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    # CIFAR10 dataset 다운로드\n",
        "    train_data = dsets.CIFAR10(root='./dataset/', train=True, transform=transforms_train, download=True)\n",
        "    val_data = dsets.CIFAR10(root=\"./dataset/\", train=False, transform=transforms_val, download=True)\n",
        "\n",
        "    return train_data, val_data\n",
        "\n",
        "def eval_data_load():\n",
        "\n",
        "    # CIFAR10 dataset 다운로드\n",
        "    transforms_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_data = dsets.CIFAR10(root='./dataset/', train=False, transform=transforms_test, download=True)\n",
        "    return test_data\n",
        "\n",
        "def test_data_load():\n",
        "    transforms_test = transforms.Compose([\n",
        "        transforms.Resize([32, 32]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    imgs = ImageFolder('./test_example', transform=transforms_test)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "def data_load(mode = \"train\"):\n",
        "    if mode.lower() == \"train\":\n",
        "        train_data, val_data = train_data_load()\n",
        "        return train_data, val_data\n",
        "\n",
        "    elif mode.lower() == \"evaluation\":\n",
        "        test_data = eval_data_load()\n",
        "        return test_data\n",
        "\n",
        "    elif mode.lower() == \"test example\":\n",
        "        test_data = test_data_load()\n",
        "        return test_data\n",
        "    \n",
        "    else:\n",
        "        print(\"Please write the correct mode [train, evaluation, test example]\")\n",
        "        exit()\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ba7e28228c4d43b581c26c727ea6d0f2",
            "8bc6caf45ffc495f979d9850decfa0ff",
            "421b63629ee44dfa80d65831825e91af",
            "798df22772de436380d449512a87fda3",
            "75fa34cd1090415bbf8061f660bd1450",
            "b63c936928d347ce8f4126ac27286c1a",
            "56a5788d7e01468ebbcfcc39c1e14249",
            "cd169ad817074a91a3b973484e73227a"
          ]
        },
        "id": "-4PKN-29KnGv",
        "outputId": "618cce32-8d5e-449a-9cfa-358c74994e39"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "import os\n",
        "\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = \"TRUE\"\n",
        "\n",
        "# configuration\n",
        "cfg = Config()\n",
        "\n",
        "print('[CIFAR10_training]')\n",
        "print('Training with:', cfg.modelname)\n",
        "# GPU 사용이 가능하면 사용하고, 불가능하면 CPU 활용\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def generate_batch(train_data, val_data):\n",
        "    train_batch_loader = DataLoader(train_data, cfg.batch_size, shuffle=True)\n",
        "    val_batch_loader = DataLoader(val_data, cfg.batch_size, shuffle=True)\n",
        "    return train_batch_loader, val_batch_loader\n",
        "\n",
        "def select_optimizer(cfg):\n",
        "    if cfg.modelname == \"LeNet5\":\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "    elif cfg.modelname == \"ResNet32\":\n",
        "        optimizer = optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
        "        decay_epoch = [32000, 48000]\n",
        "        step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                     milestones=decay_epoch, gamma=0.1)\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "def train_model(model, train_batch_loader, val_batch_loader, optimizer, criterion):\n",
        "    # training 시작\n",
        "    start_time = time.time()\n",
        "    highest_val_acc = 0\n",
        "    val_acc_list = []\n",
        "    global_steps = 0\n",
        "    epoch = 0\n",
        "    print('========================================')\n",
        "    print(\"Start training...\")\n",
        "    while True:\n",
        "        train_loss = 0\n",
        "        train_batch_cnt = 0\n",
        "        model.train()\n",
        "        for img, label in train_batch_loader:\n",
        "            global_steps += 1\n",
        "            # img.shape: [200,3,32,32]\n",
        "            # label.shape: [200]\n",
        "\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(img)\n",
        "            loss = criterion(outputs, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss\n",
        "\n",
        "            train_batch_cnt += 1\n",
        "\n",
        "            if global_steps >= cfg.finish_step:\n",
        "                print(\"Training finished.\")\n",
        "                break\n",
        "\n",
        "        ave_loss = train_loss / train_batch_cnt\n",
        "        training_time = (time.time() - start_time) / 60\n",
        "        print('========================================')\n",
        "        print(\"epoch:\", epoch + 1, \"/ global_steps:\", global_steps)\n",
        "        print(\"training dataset average loss: %.3f\" % ave_loss)\n",
        "        print(\"training_time: %.2f minutes\" % training_time)\n",
        "\n",
        "        # validation (for early stopping)\n",
        "        correct_cnt = evaluate(model, val_batch_loader, device, verbose = False)\n",
        "        \n",
        "        val_acc = correct_cnt / len(val_data) * 100\n",
        "        print(\"validation dataset accuracy: %.2f\" % val_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        if val_acc > highest_val_acc:\n",
        "            save_path = './epoch_' + str(epoch + 1) + '.pth'\n",
        "            torch.save({'epoch': epoch + 1,\n",
        "                        'model_state_dict': model.state_dict()},\n",
        "                       save_path)\n",
        "            highest_val_acc = val_acc\n",
        "        epoch += 1\n",
        "        if global_steps >= cfg.finish_step:\n",
        "            break\n",
        "\n",
        "    epoch_list = [i for i in range(1, epoch + 1)]\n",
        "    plt.title('Validation dataset accuracy plot')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(epoch_list, val_acc_list)\n",
        "    plt.show()\n",
        "    plt.savefig(\"./result.png\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 데이터 로드\n",
        "    # CIFAR10 dataset: [3,32,32] 사이즈의 이미지들을 가진 dataset\n",
        "    train_data, val_data = data_load(mode = \"train\")\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat',\n",
        "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "    # data 개수 확인\n",
        "    print('The number of training data: ', len(train_data))\n",
        "    print('The number of validation data: ', len(val_data))\n",
        "\n",
        "    # shape 및 실제 데이터 확인\n",
        "    image, label = train_data[1]\n",
        "    imgshow(image, label, classes)\n",
        "\n",
        "    # 학습 모델 생성\n",
        "    model = recall_model(cfg)\n",
        "    model.to(device)\n",
        "\n",
        "    # 배치 생성\n",
        "    train_batch_loader, val_batch_loader = generate_batch(train_data, val_data)\n",
        "\n",
        "    #loss function, optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = select_optimizer(cfg)\n",
        "\n",
        "    #Train model\n",
        "    train_model(model, train_batch_loader, val_batch_loader, optimizer, criterion)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CIFAR10_training]\n",
            "Training with: ResNet32\n",
            "GPU Available: True\n",
            "Device: cuda:0\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba7e28228c4d43b581c26c727ea6d0f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./dataset/cifar-10-python.tar.gz to ./dataset/\n",
            "Files already downloaded and verified\n",
            "The number of training data:  50000\n",
            "The number of validation data:  10000\n",
            "========================================\n",
            "The 1st image:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.8977, -0.7426, -0.6650,  ..., -0.3936,  0.0134,  0.5562],\n",
            "         [-1.0527, -0.9364, -0.5681,  ..., -0.0060,  0.3817,  0.2848],\n",
            "         [-1.1303, -1.1109, -0.8977,  ..., -0.1998,  0.2654,  0.2848],\n",
            "         ...,\n",
            "         [-0.6263, -1.2466, -1.6149,  ...,  0.5562,  0.5950,  0.9633],\n",
            "         [ 0.1104, -0.0447, -0.4324,  ...,  0.6531,  0.5950,  0.7694],\n",
            "         [ 0.3430,  0.3430,  0.3430,  ...,  0.7307,  0.6338,  0.7307]],\n",
            "\n",
            "        [[-0.8252, -0.6482, -0.5499,  ..., -0.3729,  0.2761,  1.0628],\n",
            "         [-0.9826, -0.8449, -0.4712,  ...,  0.0401,  0.5908,  0.7284],\n",
            "         [-1.0612, -1.0416, -0.8056,  ..., -0.1566,  0.4531,  0.6301],\n",
            "         ...,\n",
            "         [-0.7859, -1.3759, -1.7496,  ...,  0.7284,  0.6104,  0.8661],\n",
            "         [-0.0386, -0.1762, -0.5892,  ...,  0.7481,  0.5711,  0.6104],\n",
            "         [ 0.1974,  0.2171,  0.1974,  ...,  0.6498,  0.4924,  0.4924]],\n",
            "\n",
            "        [[-0.8557, -0.8362, -0.8362,  ..., -0.3679,  0.4320,  1.4270],\n",
            "         [-1.0313, -1.0118, -0.6996,  ...,  0.0808,  0.7832,  1.0758],\n",
            "         [-1.1483, -1.1483, -0.9727,  ..., -0.0362,  0.6856,  0.9783],\n",
            "         ...,\n",
            "         [-0.4460, -1.1093, -1.5190,  ...,  1.0954,  0.9003,  1.0173],\n",
            "         [ 0.3345,  0.1198, -0.3484,  ...,  0.5491,  0.3149,  0.2759],\n",
            "         [ 0.5881,  0.5491,  0.4905,  ...,  0.3735,  0.1589,  0.1198]]])\n",
            "Shape of this image\t: torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZrklEQVR4nO3de3RV1Z0H8O9XGsEAggjFVMEHWhlGqrBStC3LAVrQgi7RpVaoih0tTkcrttRZ1g5K1Rkf9VFtfWG14gMVfKAVpkIpjgtbsQEVUFDEAooRiAikBDXIb/64J7MC6/x2kpubcy/u72etrNzs393n7JzcX869Z5+9N80MIvLFt1exGyAi2VCyi0RCyS4SCSW7SCSU7CKRULKLRELJ/gVD8gWSF2Rdt7VITib5cDH2HQslewkjuZrkd4rdDgAgeR7JBcVuh+RPyS4FQ7JdsdsgPiX7HobkfiSfI7mR5MfJ44N2e1ofkq+Q3EryGZLdGtU/juRfSG4m+TrJIc3Y5z8BuBvAN0j+g+TmpPwBkneRnE1yG4Chu38U2P0dAcl/JjmX5CaS60lekbK/MpKPknyS5N4tPkiSSsm+59kLwO8BHAygN4DtAH6723POBfCvACoA7ABwOwCQPBDALADXAugG4GcAniTZY/edkOyd/EPobWbLAfwbgL+aWScz69roqWMB/BeAzgCCb/NJdgbwJwB/BPAVAIcDmLfbc/YBMBPApwDONLPPgkdDmk3Jvocxs4/M7EkzqzOzWuQS7V92e9pDZrbMzLYBmATgzOQt9tkAZpvZbDPbaWZzAVQBGJmyn7Vm1tXM1jbRpGfM7KVke5808dyTAHxoZjeb2SdmVmtmCxvF90XuH8EqAD8ws8+b2J60wJeK3QBpGZLlAG4FcCKA/ZLiziTbNUqO9xpVWQOgDEB35N4NnEHy5EbxMgDzW9Gk95p+yv/rhVwie45L2jPGNEKr4HRm3/NMBHAkgGPNbF8AxyflbPScXo0e9wZQD6AGucR8KDljN3x1NLPrm7FfL/l2L98GoLzRzwc0evwegMMC+5gD4DoA80j2bEabpAWU7KWvjGSHhi/kzubbAWxOLrxdlVLnbJL9kncBVwN4IjnrPwzgZJInkGyXbHNIygW+NOsBHNSMC2avATiNZDnJwwGc3yj2HIAKkpeSbE+yM8ljG1c2sxsBTEMu4bs3o13STEr20jcbueRu+OoKYB/kztQvI/cZd3cPAXgAwIcAOgC4BADM7D0ApwC4AsBG5M60lyHldZBcoPsHyd5J0Z8BvAHgQ5I1gfbeCuAz5P45TAXwSEMgucYwHMDJSdtWAhi6+wbM7BrkLtL9qXFPgrQO9dFIJA46s4tEQskuEgklu0gklOwikcj0phqSuhpYAIcc4XdV713WIbV8r738HrOO7Qs/fmXx0rdTy+2zukCtwt8w1/XQfqnlfbrt49apD2zv48A9gu+t+sAPflId2Kpj3y+7oYOO6JVavmn1amyrqWFarFXJTvJEALcBaAfgd828OUNa6Ze/vc6N9a7om1peXt47tRwABvXp6sbytc/Bw1LLP1m7OFBrS8HbMezaR1PLnxz7NbdOKC2fWOHHLjn1Sj+44prAVtO1O+4sN/aT529LLb+1stKtk/fb+ORe6zsAfBdAPwBjSKb/GxWRomvNZ/ZBAN4xs3eTkUmPIXfDhoiUoNYk+4HYdRDE+0nZLkiOJ1lFsqoV+xKRVmrzC3RmNgXAFEAX6ESKqTVn9nXYdXTVQUmZiJSg1pzZ/wbgCJKHIpfkZyE3a4kUwGkXXOLGzh1xZoYtyc/2NX9ucZ2505a4sYsnnOzG3q7x59dYMePB9MDYm9w65W4E2BLoObzizqvd2H+fFxjGXzMntfjc0WPcKnPTq2DrVn83eSe7me0geTGA55HrervfzN7Id3si0rZa9ZndzGYjNwRTREqcbpcViYSSXSQSSnaRSCjZRSKR6bRUuqmm+TRdWGF4nXL+sKDwXNfDJ7zsxno5g5AAoH8Xf5DPHf9+SGr5tyc+5dYp6z8qtfwvk7+BLX9flDrqTWd2kUgo2UUioWQXiYSSXSQSSnaRSGhhx6IaWOwGtBl+P33apPmPTHDrHOBGAP86d1joqrunTyD27m3HubHJM/x6D15/S4v3WFfjX8GvX7E0tXzHJ9vdOjqzi0RCyS4SCSW7SCSU7CKRULKLRELJLhIJDYQppooz3NBXe490Y2+9fF4bNKblDjjpXje2ftb49MDoh/0N1gU6vebc7ccmnu6Gjh99Umr54MH+5ob7IQwJxPL19VNvTi2v6DPArfPS0vSlaba8fAN2bFmjgTAiMVOyi0RCyS4SCSW7SCSU7CKRULKLREJdb22tvz9eq92qajf2v2s2u7FvdW9ViwqGDC1D5Q0B6+9XucCfcw2/Ozuwr9DYNi8WWuSp3g+N/akbOn5cDzc2MtDVN/n6z1LLjyor8ytVz0wtfvOJy7BtwzupXW+tGuJKcjWAWgCfA9hhZpWt2Z6ItJ1CjGcfamY1BdiOiLQhfWYXiURrk90AzCG5iGTq/ZEkx5OsIlnVyn2JSCu09m38YDNbR/LLAOaSXGFmLzZ+gplNATAFiPQCnUiJaNWZ3czWJd83AHgawKBCNEpECi/vrjeSHQHsZWa1yeO5AK42sz8G6ujM3kjnQGxrhl2idYHYhROmu7GHbw9Norgw7/aUtoo8Y6Fr2OPSi0dd7dYYPzq9/KlrK7FxdVXBu956AniaZMN2poUSXUSKK+9kN7N3ARxdwLaISBtS15tIJJTsIpFQsotEQskuEgmNemtj40f4kyje8/wdgZonFLQdt8z0R9HNnukvUjZv6lWBrfqj9jDQGcG2wl+/DHU/COwrQuV/dUOzNqavOTdhcCVWLk7vetOZXSQSSnaRSCjZRSKhZBeJhJJdJBK6Gl8AyxetdGN9a55xY/9xws/c2I2Z/l2+EogGrriH9P1DevkF6csxAcDx/mpYeHFqYF9XXhkIpi+tBBzr1ujW358nb9PS2wP7yk6HEZenln/68gPYuaVaV+NFYqZkF4mEkl0kEkp2kUgo2UUioWQXiYS63lpg6iPpE3+dO/Zxt85P2N6N/Tqwr4z/LoXfaPnv08uHnudW+ao/5RqqF/ix2gnLAw1ZkVra+dhT3Rq/neRvrYu/mhdGD3vED64NLV9VWGamrjeRmCnZRSKhZBeJhJJdJBJKdpFIKNlFIlGI9dm/UDZvc7qMAHQp94Zl+f1CRwb2dXLzmrRnqnMWlVrlV6kOrJBUWx/cWSCWPudd7cJFbo1xJ/mN/N5NZ7qxS2/7vhv79ZVj3ZjTRGDtXX4dXBSIpWvyzE7yfpIbSC5rVNaN5FySK5Pv+7V4zyKSqea8jX8AwIm7lV0OYJ6ZHQFgXvKziJSwJpM9WW99027FpwBomE5gKgBnTUkRKRX5fmbvaWYNU5h8iNyKrqlIjgcwPs/9iEiBtPoCnZlZ6J53M5sCYAqw598bL7Iny7frbT3JCgBIvm8oXJNEpC3ke2Z/FsA4ANcn3/1ZFYumzI1s+8DvXisv7+Jvsv57zq4GulX6d/c392qgqylbvQOxtfltssz5xcs2ulVq0cPfXmjey8AxRo3zu/Xxl+VCuX88XkgfRAcAODLw0kF94DiunekE/NdwPprT9fYogL8COJLk+yTPRy7Jh5NcCeA7yc8iUsKaPLOb2Rgn9O0Ct0VE2pBulxWJhJJdJBJKdpFIKNlFIrFHj3ob/yP/Lt177pziV6zzujoALLjPjw32+nj8UVcLAt1rwYFcmfLXNsu7683rcxzrd68dPcLf3OsLA/vqHuhGq3H6yqq9oWYABh/shtYH/p7rAyP6sCIwYyZeccqHB+q0nM7sIpFQsotEQskuEgklu0gklOwikVCyi0Rij+h6mzV9Ymr5yDNO9yutusyP1QT6TwYPDrTEGYVU7fe5hMaT9S3soKZWWJxnvVFu5KpJQ1PLjwrMaRQaNHZ3YL7GufVd3VjtNU7XYd0L/gbnBBpSPsSPrQ10AeK9QGy+U+6PpsyHzuwikVCyi0RCyS4SCSW7SCSU7CKRKJmr8QufTr/iDgCDRjsDAqpv8DfYO3Cpu0/ointo4IdztTUwoqV3YH60/fsGdpWhaWs+cGNzp/n17s9waZDhgQvds/1OASztnj7Ip+9AfxmnQYGXR2D4DF4KDIS5YFjg9ei+5PIchOTQmV0kEkp2kUgo2UUioWQXiYSSXSQSSnaRSNAsu7UWe3x5Hzvt9ENSY/fc+Z9+xbqX0svrAwNatgTWC+odGECDdwOxI9OLF852a7xw0ix/a+ljRQAAFdNLfw3Mtf7Ue+hdnl079nS3zEgv/+kZfp0Lz5ueWv7Ucz/HxppVTIs1Z/mn+0luILmsUdlkkutIvpZ8jWxqOyJSXM15G/8AgBNTym81s2OSL//UJiIloclkN7MXAWzKoC0i0oZac4HuYpJLkrf5+3lPIjmeZBXJqk+272jF7kSkNfJN9ruQu1H8GORWzr7Ze6KZTTGzSjOr7LBPydyKLxKdvJLdzNab2edmthPAvQAGFbZZIlJoeZ1qSVaYWUPf1qkAloWe36C8AzHAm3it/oVAxfR+nG/S6bMAMOVqv+/nqEnn+vvy5pkDAHyUXrx0qVujLtA9taXM31dFoBWlQt1rhRHqYvPc80D6qL1FlTe6dZpMdpKPAhgCoDvJ9wFcBWAIyWMAGIDVAC5scWtFJFNNJruZjUkpDqx+KCKlSLfLikRCyS4SCSW7SCSU7CKRyPQul/K9DAPLndkZy/wZBevO+3lqeWCeRxy1IrSYUKjPKFTPmW6wxp8YMNTGrd33hA42+aLQmV0kEkp2kUgo2UUioWQXiYSSXSQSSnaRSGTa9daxc0cMGnqsE/Unj1y2OL18SmhnQwOLgwUnlQyt5pXexbbFH/SG+sCot0FDS2SxtzbgTUbZPdDrqUF0bUtndpFIKNlFIqFkF4mEkl0kEkp2kUhkO91r+32BPsOd4KtutUFPj04PlAUGrfQeEmjIW4FYYEkpJ9Yl0IzTRwU217fwA2Fmr0gvv2zCnW6dN+dcVPB2FN6IQCz0N3O6ckpJ2bjU4sozfupW6VK9ILX87bc3unV0ZheJhJJdJBJKdpFIKNlFIqFkF4mEkl0kEs1ZEaYXgAcB9ERuBZgpZnYbyW4AHgdwCHKrwpxpZh+3SSv7DHEC2wOVQgNaQrFgQ9KLL+kd2FVgX/X5Df34zax33NglJx2R1zZ9/u920UR/KFJddXp32IA+gcE/df5yWJfcfLRfD3v4gKL6qanFVdOcflQAwMIW76Y5Z/YdACaaWT8AxwG4iGQ/AJcDmGdmRwCYl/wsIiWqyWQ3s2ozW5w8rgWwHMCBAE4B0PAvaSoA584XESkFLfrMTvIQAAOQew/Rs9FKrh8i9zZfREpUs5OdZCcATwK41My2No6ZmSH3eT6t3niSVSSrNm7cmvYUEclAs5KdZBlyif6ImT2VFK8nWZHEKwBsSKtrZlPMrNLMKnv02LcQbRaRPDSZ7CSJ3BLNy83slkahZwE03ME/DsAzhW+eiBRKc0a9fQvAOQCWknwtKbsCwPUAppM8H8AaAGc2vamdALxJ2ULdYd7osAMCdULLOO0IxPb3Q3OuSi3+0gn+8k+PBfYUmJ4O55o/Sm3MqMPdWN/lqZ+mMHwP7526cdUkNzbg8nPd2B+u+U16YNbtrW1SwVzzQfrfbGBgUOSqOenlv7q40q3TZLKb2QIAdMLfbqq+iJQG3UEnEgklu0gklOwikVCyi0RCyS4SiWwnnER7uCPH8JJfrS59cr0tC6tTywFg2WJ/xNDsGX4336zAYKLX/ZDrjDzqAMBP6HWAAB9ZelcNANSs3ZZa/s0rr3Dr/OqmX7qxhXNWubHZMx51Yy/MSe8bGjnYn4Hz9KFD3Nj7M69xY9VdAhNOLk5/7ZSSSV97Oj0w6lS3zixn2Fn7vf396MwuEgklu0gklOwikVCyi0RCyS4SCSW7SCQy7XqzHR1QX5M+/GrvHumT7sVqUyC2KjAP4dgTOrV4X4NnZDcCbO6qsW5s0LiheW3z86l35duc0lBzWnp5ICVeqFiZWl770aduHZ3ZRSKhZBeJhJJdJBJKdpFIKNlFIpHp1fiNm4m7Z/pL/EjzDBj2w2I3IW9/me4vHDRgsF9v9AX+4J/+HBbY4/xmtKpU+cuD/WJi+jyEc/+nvVtHZ3aRSCjZRSKhZBeJhJJdJBJKdpFIKNlFIkELzGcGACR7AXgQuSWZDcAUM7uN5GQAPwSwMXnqFWY2u4lthXdW8rz580IDOPw53IDA3GnBmD/3Xqk7FDe5sTFjx7ix+r7+Wkj79/Xn6+vfO728wikHwguRlQd6jusC63nVBDa6ynmJbA8MeDqgPj2Vrrv761izrir1gDSnn30HgIlmtphkZwCLSM5NYreamf/XE5GS0Zy13qqRnErMrJbkcgAHtnXDRKSwWvSZneQhAAYAaJhw+WKSS0jeT3K/ArdNRAqo2clOshOAJwFcamZbAdyF3IfYY5A789/s1BtPsopkVQHaKyJ5alaykyxDLtEfMbOnAMDM1pvZ52a2E8C9AAal1TWzKWZWaWb+wtEi0uaaTHaSBHAfgOVmdkuj8saXR08FsKzwzRORQmnO1fhvATgHwFKSryVlVwAYQ/IY5LrjVgO4sE1aWFK8brRQ95o09nekLwsFAAfM94/jjyfd4cbunvFnN3bOmVellm+C35WHLunzJALAaWec4sYqKvxt3nH7ff7+tsxILb70kiVulVNGpHc3dvQHyjXravwCAGlbDvapi0hp0R10IpFQsotEQskuEgklu0gklOwikWhy1FtBd8buBpzkREPLP/V3ype2skURGfy4H1vwvezaEbBwxEQ3Nuh5f7xVTWC0WY+O/oi4UnfyiD+4sYeuS8+jIWdX4tU300e96cwuEgklu0gklOwikVCyi0RCyS4SCSW7SCQyXesN+AjhLjZPll1sPwrE7sqsFQW39K1AcGwgNq3ADfGHZd2w1J9kc8w0fwTYK+gS2N8FTvniQJ21gVhwOspArHsgdqyzOX97ry79ILW8bnu9W0dndpFIKNlFIqFkF4mEkl0kEkp2kUgo2UUikXHX255gfrEb0Da2hLoNs1s77quj/JFcP540zI3V+z1K2HdxejcUACxYcm9qebk/pyRWBXrlygK9wGud9dcA4C34Q/MO69Mxtbx8rf97DRr8ldTyjp38xeh0ZheJhJJdJBJKdpFIKNlFIqFkF4lEk1fjSXYA8CKA9snznzCzq0geCuAxAPsDWATgHDP7LLStMuyHnvhOauz9/18FOk1oYEKhrchwX1nK7op7yNtLn3BjQ471r8aHrFrovz4Wzkz/e07+nb+gUe3a9OWYcvJ7LR7UZ6gbu/HeUanlW+oWuHVenZ9+db+udqVbpzln9k8BDDOzo5FbnvlEkscBuAHArWZ2OICPAZzfjG2JSJE0meyW84/kx7LkywAMA9Dwb3oqgNFt0kIRKYjmrs/eLlnBdQOAucgtW7rZzHYkT3kfwIFt00QRKYRmJbuZfW5mxwA4CMAgAIH7j3ZFcjzJKpJVO/Fpns0UkdZq0dV4M9uM3P2k3wDQlWTDBb6DAKxz6kwxs0ozq9wL7VvVWBHJX5PJTrIHya7J430ADAewHLmkPz152jgAz7RVI0Wk9ZozEKYCwFSS7ZD75zDdzJ4j+SaAx0heC+BVAPc1taFO5Z3xrb5D0oNlzjxcAFCfPu/Xli3+fGBrA7E3a0LdfF/UrrcSsdYfkEMWfo6/fs6Il9q6bP/Owyv8gTDdq9NH19TX+G2scwbW7PzsE7dOk8luZksADEgpfxe5z+8isgfQHXQikVCyi0RCyS4SCSW7SCSU7CKRoJk/b1bBd0ZuBLAm+bE7AH+9n+yoHbtSO3a1p7XjYDPrkRbINNl32TFZZWaVRdm52qF2RNgOvY0XiYSSXSQSxUz2KUXcd2Nqx67Ujl19YdpRtM/sIpItvY0XiYSSXSQSRUl2kieSfIvkOyQvL0YbknasJrmU5GskqzLc7/0kN5Bc1qisG8m5JFcm3/crUjsmk1yXHJPXSI7MoB29SM4n+SbJN0hOSMozPSaBdmR6TEh2IPkKydeTdvwyKT+U5MIkbx4nuXeLNmxmmX4BaIfcHHaHAdgbwOsA+mXdjqQtqwF0L8J+jwcwEMCyRmU3Arg8eXw5gBuK1I7JAH6W8fGoADAwedwZwNsA+mV9TALtyPSYACCATsnjMgALARwHYDqAs5LyuwH8qCXbLcaZfRCAd8zsXcvNM/8YgFOK0I6iMbMXAWzarfgU5GbpBTKarddpR+bMrNrMFiePa5GbCelAZHxMAu3IlOUUfEbnYiT7gQDea/RzMWemNQBzSC4iOb5IbWjQ08waVnL4EEDPIrblYpJLkrf5bf5xojGShyA3WcpCFPGY7NYOIONj0hYzOsd+gW6wmQ0E8F0AF5E8vtgNAnL/2ZH7R1QMdwHog9yCINUAbs5qxyQ7AXgSwKVmtrVxLMtjktKOzI+JtWJGZ08xkn0dgF6NfnZnpm1rZrYu+b4BwNMo7jRb60lWAEDyfUMxGmFm65MX2k4A9yKjY0KyDLkEe8TMnkqKMz8mae0o1jFJ9t3iGZ09xUj2vwE4IrmyuDeAswA8m3UjSHYk2bnhMYARAJaFa7WpZ5GbpRco4my9DcmVOBUZHBOSRG7C0uVmdkujUKbHxGtH1sekzWZ0zuoK425XG0cid6VzFYBfFKkNhyHXE/A6gDeybAeAR5F7O1iP3Gev85FbIHMegJUA/gSgW5Ha8RCApQCWIJdsFRm0YzByb9GXAHgt+RqZ9TEJtCPTYwLga8jN2LwEuX8sVzZ6zb4C4B0AMwC0b8l2dbusSCRiv0AnEg0lu0gklOwikVCyi0RCyS4SCSW7SCSU7CKR+D/bH69d9aoargAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label of this image: 9 truck\n",
            "========================================\n",
            "Start training...\n",
            "========================================\n",
            "epoch: 1 / global_steps: 391\n",
            "training dataset average loss: 1.860\n",
            "training_time: 0.34 minutes\n",
            "validation dataset accuracy: 45.15\n",
            "========================================\n",
            "epoch: 2 / global_steps: 782\n",
            "training dataset average loss: 1.350\n",
            "training_time: 0.72 minutes\n",
            "validation dataset accuracy: 56.65\n",
            "========================================\n",
            "epoch: 3 / global_steps: 1173\n",
            "training dataset average loss: 1.019\n",
            "training_time: 1.10 minutes\n",
            "validation dataset accuracy: 63.54\n",
            "========================================\n",
            "epoch: 4 / global_steps: 1564\n",
            "training dataset average loss: 0.827\n",
            "training_time: 1.49 minutes\n",
            "validation dataset accuracy: 70.93\n",
            "========================================\n",
            "epoch: 5 / global_steps: 1955\n",
            "training dataset average loss: 0.718\n",
            "training_time: 1.88 minutes\n",
            "validation dataset accuracy: 73.95\n",
            "========================================\n",
            "epoch: 6 / global_steps: 2346\n",
            "training dataset average loss: 0.636\n",
            "training_time: 2.27 minutes\n",
            "validation dataset accuracy: 72.54\n",
            "========================================\n",
            "epoch: 7 / global_steps: 2737\n",
            "training dataset average loss: 0.585\n",
            "training_time: 2.66 minutes\n",
            "validation dataset accuracy: 76.74\n",
            "========================================\n",
            "epoch: 8 / global_steps: 3128\n",
            "training dataset average loss: 0.546\n",
            "training_time: 3.06 minutes\n",
            "validation dataset accuracy: 78.08\n",
            "========================================\n",
            "epoch: 9 / global_steps: 3519\n",
            "training dataset average loss: 0.517\n",
            "training_time: 3.45 minutes\n",
            "validation dataset accuracy: 77.49\n",
            "========================================\n",
            "epoch: 10 / global_steps: 3910\n",
            "training dataset average loss: 0.484\n",
            "training_time: 3.85 minutes\n",
            "validation dataset accuracy: 74.02\n",
            "========================================\n",
            "epoch: 11 / global_steps: 4301\n",
            "training dataset average loss: 0.464\n",
            "training_time: 4.25 minutes\n",
            "validation dataset accuracy: 80.71\n",
            "========================================\n",
            "epoch: 12 / global_steps: 4692\n",
            "training dataset average loss: 0.441\n",
            "training_time: 4.65 minutes\n",
            "validation dataset accuracy: 81.43\n",
            "========================================\n",
            "epoch: 13 / global_steps: 5083\n",
            "training dataset average loss: 0.424\n",
            "training_time: 5.05 minutes\n",
            "validation dataset accuracy: 81.85\n",
            "========================================\n",
            "epoch: 14 / global_steps: 5474\n",
            "training dataset average loss: 0.412\n",
            "training_time: 5.45 minutes\n",
            "validation dataset accuracy: 81.83\n",
            "========================================\n",
            "epoch: 15 / global_steps: 5865\n",
            "training dataset average loss: 0.398\n",
            "training_time: 5.85 minutes\n",
            "validation dataset accuracy: 80.47\n",
            "========================================\n",
            "epoch: 16 / global_steps: 6256\n",
            "training dataset average loss: 0.383\n",
            "training_time: 6.25 minutes\n",
            "validation dataset accuracy: 84.41\n",
            "========================================\n",
            "epoch: 17 / global_steps: 6647\n",
            "training dataset average loss: 0.373\n",
            "training_time: 6.66 minutes\n",
            "validation dataset accuracy: 80.79\n",
            "========================================\n",
            "epoch: 18 / global_steps: 7038\n",
            "training dataset average loss: 0.360\n",
            "training_time: 7.06 minutes\n",
            "validation dataset accuracy: 83.27\n",
            "========================================\n",
            "epoch: 19 / global_steps: 7429\n",
            "training dataset average loss: 0.358\n",
            "training_time: 7.46 minutes\n",
            "validation dataset accuracy: 84.31\n",
            "========================================\n",
            "epoch: 20 / global_steps: 7820\n",
            "training dataset average loss: 0.348\n",
            "training_time: 7.86 minutes\n",
            "validation dataset accuracy: 82.40\n",
            "========================================\n",
            "epoch: 21 / global_steps: 8211\n",
            "training dataset average loss: 0.333\n",
            "training_time: 8.26 minutes\n",
            "validation dataset accuracy: 85.30\n",
            "========================================\n",
            "epoch: 22 / global_steps: 8602\n",
            "training dataset average loss: 0.325\n",
            "training_time: 8.66 minutes\n",
            "validation dataset accuracy: 83.51\n",
            "========================================\n",
            "epoch: 23 / global_steps: 8993\n",
            "training dataset average loss: 0.323\n",
            "training_time: 9.06 minutes\n",
            "validation dataset accuracy: 83.03\n",
            "========================================\n",
            "epoch: 24 / global_steps: 9384\n",
            "training dataset average loss: 0.319\n",
            "training_time: 9.47 minutes\n",
            "validation dataset accuracy: 83.15\n",
            "========================================\n",
            "epoch: 25 / global_steps: 9775\n",
            "training dataset average loss: 0.306\n",
            "training_time: 9.87 minutes\n",
            "validation dataset accuracy: 83.66\n",
            "========================================\n",
            "epoch: 26 / global_steps: 10166\n",
            "training dataset average loss: 0.306\n",
            "training_time: 10.27 minutes\n",
            "validation dataset accuracy: 85.27\n",
            "========================================\n",
            "epoch: 27 / global_steps: 10557\n",
            "training dataset average loss: 0.297\n",
            "training_time: 10.67 minutes\n",
            "validation dataset accuracy: 85.57\n",
            "========================================\n",
            "epoch: 28 / global_steps: 10948\n",
            "training dataset average loss: 0.294\n",
            "training_time: 11.07 minutes\n",
            "validation dataset accuracy: 85.41\n",
            "========================================\n",
            "epoch: 29 / global_steps: 11339\n",
            "training dataset average loss: 0.291\n",
            "training_time: 11.48 minutes\n",
            "validation dataset accuracy: 83.49\n",
            "========================================\n",
            "epoch: 30 / global_steps: 11730\n",
            "training dataset average loss: 0.284\n",
            "training_time: 11.88 minutes\n",
            "validation dataset accuracy: 81.80\n",
            "========================================\n",
            "epoch: 31 / global_steps: 12121\n",
            "training dataset average loss: 0.286\n",
            "training_time: 12.28 minutes\n",
            "validation dataset accuracy: 85.28\n",
            "========================================\n",
            "epoch: 32 / global_steps: 12512\n",
            "training dataset average loss: 0.279\n",
            "training_time: 12.68 minutes\n",
            "validation dataset accuracy: 84.65\n",
            "========================================\n",
            "epoch: 33 / global_steps: 12903\n",
            "training dataset average loss: 0.277\n",
            "training_time: 13.09 minutes\n",
            "validation dataset accuracy: 84.05\n",
            "========================================\n",
            "epoch: 34 / global_steps: 13294\n",
            "training dataset average loss: 0.269\n",
            "training_time: 13.49 minutes\n",
            "validation dataset accuracy: 87.03\n",
            "========================================\n",
            "epoch: 35 / global_steps: 13685\n",
            "training dataset average loss: 0.268\n",
            "training_time: 13.89 minutes\n",
            "validation dataset accuracy: 85.06\n",
            "========================================\n",
            "epoch: 36 / global_steps: 14076\n",
            "training dataset average loss: 0.264\n",
            "training_time: 14.29 minutes\n",
            "validation dataset accuracy: 83.63\n",
            "========================================\n",
            "epoch: 37 / global_steps: 14467\n",
            "training dataset average loss: 0.256\n",
            "training_time: 14.70 minutes\n",
            "validation dataset accuracy: 85.44\n",
            "========================================\n",
            "epoch: 38 / global_steps: 14858\n",
            "training dataset average loss: 0.263\n",
            "training_time: 15.10 minutes\n",
            "validation dataset accuracy: 84.93\n",
            "Training finished.\n",
            "========================================\n",
            "epoch: 39 / global_steps: 15000\n",
            "training dataset average loss: 0.246\n",
            "training_time: 15.27 minutes\n",
            "validation dataset accuracy: 84.02\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkHKrSPWKqXf",
        "outputId": "79a4afa7-2b1d-41ab-aab6-f958706144d0"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from CIFAR10_configuration import Config\n",
        "# from model.LeNet5_model import LeNet5_model\n",
        "# from model.ResNet_model import ResNet32_model\n",
        "# from utils import *\n",
        "\n",
        "print('[CIFAR10_evaluation]')\n",
        "cfg = Config()\n",
        "# GPU 사용이 가능하면 사용하고, 불가능하면 CPU 활용\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "def generate_batch(test_data):\n",
        "    test_batch_loader = DataLoader(test_data, cfg.batch_size, shuffle=True)\n",
        "    return test_batch_loader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 데이터 로드\n",
        "    test_data = data_load()\n",
        "    test_data = data_load(mode = \"evaluation\")\n",
        "    # data 개수 확인\n",
        "    print('The number of test data: ', len(test_data))\n",
        "    # 배치 생성\n",
        "    test_batch_loader = generate_batch(test_data)\n",
        "\n",
        "    # test 시작\n",
        "    acc_list = []\n",
        "\n",
        "    #TODO\n",
        "    #save_path 수정하기\n",
        "    save_path = \"./epoch_2.pth\"\n",
        "\n",
        "    # 저장된 model 불러오기\n",
        "    model, checkpoint = recall_model(cfg, save_path = save_path)\n",
        "    model.to(device)\n",
        "\n",
        "    correct_cnt = evaluate(model, test_batch_loader, device, verbose = False)\n",
        "\n",
        "    accuracy = correct_cnt / len(test_data) * 100\n",
        "    print(\"accuracy of the %d epoch trained model : %.2f%%\"%(checkpoint[\"epoch\"], accuracy))\n",
        "    acc_list.append(accuracy)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CIFAR10_evaluation]\n",
            "GPU Available: True\n",
            "device: cuda:0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "The number of test data:  10000\n",
            "accuracy of the 2 epoch trained model : 56.65%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xokoz0RzPAY4",
        "outputId": "0d1e4b0b-c2c5-45b7-929a-d834af8f87a3"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "print('[CIFAR10_evaluation]')\n",
        "cfg = Config()\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# GPU 사용이 가능하면 사용하고, 불가능하면 CPU 활용\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #test_example에서 이미지 불러오기\n",
        "    imgs = data_load(mode = \"test example\")\n",
        "    test_loader = DataLoader(imgs, batch_size=1)\n",
        "\n",
        "    # 저장된 state 불러오기\n",
        "    #TODO\n",
        "    #save_path 바꾸기\n",
        "    save_path = \"./epoch_3.pth\"\n",
        "    #모델 불러오기\n",
        "    model, checkpoint = recall_model(cfg, save_path)\n",
        "    model.to(device)\n",
        "\n",
        "    #평가\n",
        "    evaluate(model, test_loader, device, verbose = True)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CIFAR10_evaluation]\n",
            "GPU Available: True\n",
            "device: cuda:0\n",
            "--------------------------------------\n",
            "truth: car\n",
            "model prediction: car\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDsS_YmhPbz9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}